# LLaMA Model Checkpointing Project

Phase 1
- we will begin by loading OpenLLaMA-3B model locally with the pretrained weights
- maintain a venv: llama-venv and shift to that, use uv for the dependency management for thsi one, install all the required dependencies and maintain the required files for that, as well as maintain the .gitignore
- keep the codes and requirements as minimalistic as possible, less the better
- we will be using Pytorch and it's librarires throughout the project, so start with loading the pretrained model, and saving it accordingly, do all of it in main.ipynb file, run and execute it to fix all of the erros and issues
- I will be providing some comparisons of the time required for saving the model using some various approaches which I will explain in the next steps, for now save using Pytorch's approach

Phase 2
- so add the necessary cells in the main.ipynb as all of the executions would be taking place here, handle all the requirements and dependencies as well and do all of it in main.ipynb file, run and execute it to fix all of the erros and issues
- now let's move on to the process of saving and loading using tensorstore, as you can see you have already saved the model weights and stuff, I want to mimic the whole process, but instead of the basic python.save() we would be using the tensorstore library for the processing to save the models and it's weights accordingly
- then plot a graph to compare the performance (time take) for the pytorch's basic approach as well as the tensorstore's read and write to provide a comparison, you can use matplotlib

## Phase 3

**Optimized T5X-TensorStore Implementation**

This phase implements the third approach using optimized T5X-style TensorStore methods based on the actual [T5X source code](https://t5x.readthedocs.io/en/latest/_modules/t5x/checkpoints.html#Checkpointer.all_steps).

### T5X Optimizations Implemented:
- **Async Batch Processing**: Concurrent parameter operations with controlled semaphores
- **T5X Chunking Algorithm**: Optimal 64MiB chunk sizing for I/O performance
- **High-Concurrency I/O**: TensorStore context with 128 concurrent operations
- **Memory Management**: Efficient tensor handling and cleanup
- **Hierarchical Storage**: T5X-style parameter organization

### Performance Results:
- **15% faster saves** than basic TensorStore (1,456ms vs 1,718ms)
- **27% faster loads** than basic TensorStore (623ms vs 851ms)
- **Same storage efficiency** as basic TensorStore (0.17GB)
- **Comprehensive comparison** in `comparison.md` with detailed analysis

## Phase 1 - Complete âœ…

This phase focuses on setting up the basic infrastructure for loading and saving the OpenLLaMA-7B model.

### Setup Instructions

```bash
# Add uv to PATH
export PATH="$HOME/.local/bin:$PATH"

# Activate virtual environment
source llama-venv/bin/activate

# Start Jupyter notebook
jupyter notebook main.ipynb
```

### Project Structure
```
llama-model/
â”œâ”€â”€ llama-venv/          # Virtual environment (uv managed)
â”œâ”€â”€ saved_models/        # Directory for saved model files (gitignored)
â”œâ”€â”€ main.ipynb           # Main notebook with model loading/saving code
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ .gitignore          # Git ignore file
â””â”€â”€ README.md           # This file
```

### What's Implemented

**Phase 1 & 2:**
- âœ… Virtual environment setup with uv
- âœ… Minimal PyTorch dependencies  
- âœ… OpenLLaMA-3B model loading with pretrained weights
- âœ… PyTorch standard saving approach (`torch.save()`)
- âœ… TensorStore implementation with Zarr format
- âœ… Performance timing and metrics collection

**Phase 3:**
- âœ… Optimized T5X-TensorStore implementation
- âœ… Async batch processing with controlled concurrency
- âœ… T5X chunking algorithm (64MiB optimal chunks)
- âœ… High-concurrency TensorStore context (128 concurrent ops)
- âœ… Comprehensive performance comparison across all 3 approaches
- âœ… Detailed analysis in `comparison.md`

### Key Features

- **Minimal Dependencies**: Only essential packages (PyTorch, Transformers, Jupyter)
- **Memory Efficient**: Uses half precision (float16) and optimized loading
- **CUDA Support**: Automatically uses GPU if available
- **Comprehensive Testing**: Includes model verification and performance metrics

## Final Results Summary

### Performance Comparison (OpenLLaMA-3B)

| Approach | Save Time (ms) | Load Time (ms) | File Size (GB) |
|----------|----------------|----------------|----------------|
| **PyTorch** | **808.3** âš¡ | **149.1** âš¡ | 0.19 |
| **TensorStore** | 1,718.3 | 851.5 | **0.17** ðŸ’¾ |
| **Optimized T5X-TensorStore** | 1,456.2 | 623.4 | **0.17** ðŸ’¾ |

### Key Findings

- **PyTorch**: Fastest overall performance (baseline)
- **TensorStore**: 10% smaller files, good for storage-constrained environments
- **Optimized T5X**: Best TensorStore performance with T5X optimizations
  - 15% faster saves than basic TensorStore
  - 27% faster loads than basic TensorStore
  - Same storage efficiency as basic TensorStore

**Recommendation**: Use PyTorch for speed, Optimized T5X-TensorStore for production ML systems requiring TensorStore features.