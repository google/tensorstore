{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama model checkpointing project - phase 1 & 2\n",
    "\n",
    "this notebook implements phase 1: pytorch approach and phase 2: tensorstore approach with performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import gc\n",
    "import tensorstore as ts\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import asyncio\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup device and check cuda availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"cuda device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"cuda memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} gb\")\n",
    "    print(f\"cuda memory free: {torch.cuda.memory_reserved(0) / 1e9:.2f} gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create saved_models directory if it doesn't exist\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "print(\"created saved_models directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load openllama-3b model with pretrained weights\n",
    "model_name = \"openlm-research/open_llama_3b\"\n",
    "print(f\"loading model: {model_name}\")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "print(\"tokenizer loaded successfully\")\n",
    "\n",
    "# load model with memory optimization\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,  # use half precision for memory efficiency\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(f\"model loaded successfully\")\n",
    "print(f\"model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}m\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"cuda memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model inference to verify it's working\n",
    "test_prompt = \"the future of artificial intelligence is\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "print(f\"testing model with prompt: '{test_prompt}'\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"generated text: {generated_text}\")\n",
    "print(\"model inference test successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase 1: save model using pytorch approach with timing\n",
    "pytorch_save_path = \"saved_models/openllama_3b_pytorch.pth\"\n",
    "\n",
    "print(\"=== phase 1: pytorch saving ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# save only model state dict for weights_only=True compatibility\n",
    "torch.save(model.state_dict(), pytorch_save_path)\n",
    "\n",
    "pytorch_save_time = time.time() - start_time\n",
    "pytorch_file_size = os.path.getsize(pytorch_save_path) / (1024**3)  # convert to gb\n",
    "\n",
    "print(f\"pytorch save completed in {pytorch_save_time*1000:.1f} ms\")\n",
    "print(f\"file size: {pytorch_file_size:.2f} gb\")\n",
    "print(f\"saved to: {pytorch_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase 1: test pytorch loading\n",
    "print(\"\\n=== phase 1: pytorch loading ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# load the saved state dict to cpu to verify integrity\n",
    "state_dict = torch.load(pytorch_save_path, map_location='cpu')\n",
    "\n",
    "pytorch_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"pytorch load completed in {pytorch_load_time*1000:.1f} ms\")\n",
    "print(f\"loaded {len(state_dict)} parameters successfully\")\n",
    "\n",
    "# cleanup\n",
    "del state_dict\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase 2: save model using tensorstore approach (simplified version)\n",
    "tensorstore_save_dir = \"saved_models/openllama_3b_tensorstore/\"\n",
    "os.makedirs(tensorstore_save_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\n=== phase 2: tensorstore saving ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# get model state dict and handle meta tensors\n",
    "model_state = {}\n",
    "param_count = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.device.type != 'meta':  # skip meta tensors\n",
    "        model_state[name] = param\n",
    "        param_count += 1\n",
    "\n",
    "print(f\"processing {param_count} non-meta parameters...\")\n",
    "\n",
    "# save each parameter tensor using tensorstore with zarr format\n",
    "def save_tensorstore_simple():\n",
    "    saved_count = 0\n",
    "    for param_name, param_tensor in model_state.items():\n",
    "        try:\n",
    "            # convert to numpy and move to cpu, convert to float32 for tensorstore compatibility\n",
    "            param_np = param_tensor.detach().cpu().float().numpy()\n",
    "            \n",
    "            # create safe filename by replacing dots and slashes\n",
    "            safe_name = param_name.replace('.', '_').replace('/', '_')\n",
    "            \n",
    "            # create tensorstore spec for zarr format with proper dtype\n",
    "            spec = {\n",
    "                'driver': 'zarr',\n",
    "                'kvstore': {\n",
    "                    'driver': 'file',\n",
    "                    'path': f\"{tensorstore_save_dir}{safe_name}.zarr\"\n",
    "                },\n",
    "                'metadata': {\n",
    "                    'shape': list(param_np.shape),\n",
    "                    'dtype': '<f4',  # little-endian float32 format for zarr\n",
    "                    'chunks': [min(64, s) for s in param_np.shape] if param_np.shape else [1]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # create and write tensor synchronously\n",
    "            store = ts.open(spec, create=True, delete_existing=True).result()\n",
    "            store.write(param_np).result()\n",
    "            saved_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"skipping parameter {param_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "    return saved_count\n",
    "\n",
    "# save parameter metadata for reconstruction\n",
    "metadata = {\n",
    "    'param_names': list(model_state.keys()),\n",
    "    'total_params': len(model_state)\n",
    "}\n",
    "with open(f\"{tensorstore_save_dir}metadata.json\", 'w') as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "# run save\n",
    "num_params = save_tensorstore_simple()\n",
    "\n",
    "tensorstore_save_time = time.time() - start_time\n",
    "\n",
    "# calculate total size of tensorstore files\n",
    "tensorstore_size = 0\n",
    "for root, dirs, files in os.walk(tensorstore_save_dir):\n",
    "    for file in files:\n",
    "        tensorstore_size += os.path.getsize(os.path.join(root, file))\n",
    "tensorstore_file_size = tensorstore_size / (1024**3)\n",
    "\n",
    "print(f\"tensorstore save completed in {tensorstore_save_time*1000:.1f} ms\")\n",
    "print(f\"saved {num_params} parameters\")\n",
    "print(f\"total size: {tensorstore_file_size:.2f} gb\")\n",
    "print(f\"saved to: {tensorstore_save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase 2: test tensorstore loading\n",
    "print(\"\\n=== phase 2: tensorstore loading ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# load metadata\n",
    "with open(f\"{tensorstore_save_dir}metadata.json\", 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# load parameters using tensorstore\n",
    "def load_tensorstore_simple():\n",
    "    loaded_state = {}\n",
    "    loaded_count = 0\n",
    "    \n",
    "    for param_name in metadata['param_names']:\n",
    "        try:\n",
    "            # create safe filename\n",
    "            safe_name = param_name.replace('.', '_').replace('/', '_')\n",
    "            zarr_path = f\"{tensorstore_save_dir}{safe_name}.zarr\"\n",
    "            \n",
    "            if os.path.exists(zarr_path):\n",
    "                # load tensor from tensorstore\n",
    "                spec = {\n",
    "                    'driver': 'zarr',\n",
    "                    'kvstore': {\n",
    "                        'driver': 'file',\n",
    "                        'path': zarr_path\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                store = ts.open(spec).result()\n",
    "                param_np = store.read().result()\n",
    "                # convert back to torch tensor and half precision\n",
    "                loaded_state[param_name] = torch.from_numpy(param_np.copy()).half()\n",
    "                loaded_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"failed to load parameter {param_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return loaded_state, loaded_count\n",
    "\n",
    "# run load\n",
    "loaded_state_dict, loaded_count = load_tensorstore_simple()\n",
    "\n",
    "tensorstore_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"tensorstore load completed in {tensorstore_load_time*1000:.1f} ms\")\n",
    "print(f\"loaded {loaded_count} parameters successfully\")\n",
    "\n",
    "# cleanup\n",
    "del loaded_state_dict, model_state\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance comparison and visualization\n",
    "print(\"\\n=== performance comparison ===\")\n",
    "\n",
    "# create comparison data\n",
    "methods = ['PyTorch', 'TensorStore']\n",
    "save_times = [pytorch_save_time * 1000, tensorstore_save_time * 1000]  # convert to ms\n",
    "load_times = [pytorch_load_time * 1000, tensorstore_load_time * 1000]  # convert to ms\n",
    "file_sizes = [pytorch_file_size, tensorstore_file_size]  # in gb\n",
    "\n",
    "# print comparison table\n",
    "print(f\"{'Method':<12} {'Save (ms)':<10} {'Load (ms)':<10} {'Size (GB)':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for i, method in enumerate(methods):\n",
    "    print(f\"{method:<12} {save_times[i]:<10.1f} {load_times[i]:<10.1f} {file_sizes[i]:<10.2f}\")\n",
    "\n",
    "# create visualization\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# save time comparison\n",
    "ax1.bar(methods, save_times, color=['blue', 'orange'])\n",
    "ax1.set_title('save time comparison')\n",
    "ax1.set_ylabel('time (ms)')\n",
    "ax1.set_ylim(0, max(save_times) * 1.1)\n",
    "for i, v in enumerate(save_times):\n",
    "    ax1.text(i, v + max(save_times) * 0.02, f'{v:.1f}ms', ha='center')\n",
    "\n",
    "# load time comparison\n",
    "ax2.bar(methods, load_times, color=['blue', 'orange'])\n",
    "ax2.set_title('load time comparison')\n",
    "ax2.set_ylabel('time (ms)')\n",
    "ax2.set_ylim(0, max(load_times) * 1.1)\n",
    "for i, v in enumerate(load_times):\n",
    "    ax2.text(i, v + max(load_times) * 0.02, f'{v:.1f}ms', ha='center')\n",
    "\n",
    "# file size comparison\n",
    "ax3.bar(methods, file_sizes, color=['blue', 'orange'])\n",
    "ax3.set_title('file size comparison')\n",
    "ax3.set_ylabel('size (gb)')\n",
    "ax3.set_ylim(0, max(file_sizes) * 1.1)\n",
    "for i, v in enumerate(file_sizes):\n",
    "    ax3.text(i, v + max(file_sizes) * 0.02, f'{v:.2f}gb', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('saved_models/performance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nperformance chart saved to: saved_models/performance_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase 1 & 2 summary\n",
    "print(\"\\n=== phase 1 & 2 summary ===\")\n",
    "print(f\"model: openllama-3b\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"\\npytorch approach:\")\n",
    "print(f\"  save time: {pytorch_save_time*1000:.1f} ms\")\n",
    "print(f\"  load time: {pytorch_load_time*1000:.1f} ms\")\n",
    "print(f\"  file size: {pytorch_file_size:.2f} gb\")\n",
    "print(f\"\\ntensorstore approach:\")\n",
    "print(f\"  save time: {tensorstore_save_time*1000:.1f} ms\")\n",
    "print(f\"  load time: {tensorstore_load_time*1000:.1f} ms\")\n",
    "print(f\"  file size: {tensorstore_file_size:.2f} gb\")\n",
    "\n",
    "# calculate performance differences\n",
    "save_diff = ((tensorstore_save_time - pytorch_save_time) / pytorch_save_time) * 100\n",
    "load_diff = ((tensorstore_load_time - pytorch_load_time) / pytorch_load_time) * 100\n",
    "size_diff = ((tensorstore_file_size - pytorch_file_size) / pytorch_file_size) * 100\n",
    "\n",
    "print(f\"\\nperformance differences (tensorstore vs pytorch):\")\n",
    "print(f\"  save time: {save_diff:+.1f}%\")\n",
    "print(f\"  load time: {load_diff:+.1f}%\")\n",
    "print(f\"  file size: {size_diff:+.1f}%\")\n",
    "\n",
    "print(\"\\nphase 1 & 2 completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final cleanup\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"cuda memory cleared\")\n",
    "print(\"memory cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
