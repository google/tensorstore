{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama model checkpointing project - phase 1\n",
    "\n",
    "this notebook implements phase 1: loading openllama-3b model and saving with pytorch approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T06:44:23.952687Z",
     "iopub.status.busy": "2025-10-09T06:44:23.952123Z",
     "iopub.status.idle": "2025-10-09T06:44:29.867399Z",
     "shell.execute_reply": "2025-10-09T06:44:29.866817Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T06:44:29.869236Z",
     "iopub.status.busy": "2025-10-09T06:44:29.868986Z",
     "iopub.status.idle": "2025-10-09T06:44:29.915670Z",
     "shell.execute_reply": "2025-10-09T06:44:29.915123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "cuda device: NVIDIA GeForce GTX 1650\n",
      "cuda memory: 3.9 gb\n",
      "cuda memory free: 0.00 gb\n"
     ]
    }
   ],
   "source": [
    "# setup device and check cuda availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"cuda device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"cuda memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} gb\")\n",
    "    print(f\"cuda memory free: {torch.cuda.memory_reserved(0) / 1e9:.2f} gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T06:44:29.948156Z",
     "iopub.status.busy": "2025-10-09T06:44:29.947947Z",
     "iopub.status.idle": "2025-10-09T06:44:29.950625Z",
     "shell.execute_reply": "2025-10-09T06:44:29.950245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created saved_models directory\n"
     ]
    }
   ],
   "source": [
    "# create saved_models directory if it doesn't exist\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "print(\"created saved_models directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T06:44:29.952070Z",
     "iopub.status.busy": "2025-10-09T06:44:29.951914Z",
     "iopub.status.idle": "2025-10-09T06:44:37.804195Z",
     "shell.execute_reply": "2025-10-09T06:44:37.803747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model: openlm-research/open_llama_3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded successfully\n",
      "model parameters: 3426.5m\n",
      "cuda memory allocated: 2.93 gb\n"
     ]
    }
   ],
   "source": [
    "# load openllama-3b model with pretrained weights\n",
    "model_name = \"openlm-research/open_llama_3b\"\n",
    "print(f\"loading model: {model_name}\")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "print(\"tokenizer loaded successfully\")\n",
    "\n",
    "# load model with memory optimization (fixed: use dtype instead of torch_dtype)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,  # use half precision for memory efficiency\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(f\"model loaded successfully\")\n",
    "print(f\"model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}m\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"cuda memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T06:44:37.806092Z",
     "iopub.status.busy": "2025-10-09T06:44:37.805894Z",
     "iopub.status.idle": "2025-10-09T06:45:37.446095Z",
     "shell.execute_reply": "2025-10-09T06:45:37.433502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model with prompt: 'the future of artificial intelligence is'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text: the future of artificial intelligence is now #ai\n",
      "Estonia has 100,000 drones and 1,000,000 people. It’s the world’s most advanced country at using them\n",
      "model inference test successful\n"
     ]
    }
   ],
   "source": [
    "# test model inference to verify it's working\n",
    "test_prompt = \"the future of artificial intelligence is\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "print(f\"testing model with prompt: '{test_prompt}'\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"generated text: {generated_text}\")\n",
    "print(\"model inference test successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T06:45:37.553530Z",
     "iopub.status.busy": "2025-10-09T06:45:37.552460Z",
     "iopub.status.idle": "2025-10-09T06:45:43.955547Z",
     "shell.execute_reply": "2025-10-09T06:45:43.954784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model using pytorch approach...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch save completed in 6377.5 ms\n",
      "file size: 2.73 gb\n",
      "saved to: saved_models/openllama_3b_pytorch.pth\n"
     ]
    }
   ],
   "source": [
    "# save model using pytorch approach with timing (save only state dict for security)\n",
    "pytorch_save_path = \"saved_models/openllama_3b_pytorch.pth\"\n",
    "\n",
    "print(\"saving model using pytorch approach...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# save only model state dict for weights_only=True compatibility\n",
    "torch.save(model.state_dict(), pytorch_save_path)\n",
    "\n",
    "pytorch_save_time = time.time() - start_time\n",
    "file_size = os.path.getsize(pytorch_save_path) / (1024**3)  # convert to gb\n",
    "\n",
    "print(f\"pytorch save completed in {pytorch_save_time*1000:.1f} ms\")\n",
    "print(f\"file size: {file_size:.2f} gb\")\n",
    "print(f\"saved to: {pytorch_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T06:45:43.963180Z",
     "iopub.status.busy": "2025-10-09T06:45:43.962689Z",
     "iopub.status.idle": "2025-10-09T06:45:49.565462Z",
     "shell.execute_reply": "2025-10-09T06:45:49.564610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model loading from saved file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch load completed in 3616.3 ms\n",
      "loaded 237 parameters successfully\n",
      "model loading test successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "697"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simplified load test - just verify file can be loaded\n",
    "print(\"testing model loading from saved file...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# load the saved state dict to cpu to verify integrity\n",
    "state_dict = torch.load(pytorch_save_path, map_location='cpu')\n",
    "\n",
    "pytorch_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"pytorch load completed in {pytorch_load_time*1000:.1f} ms\")\n",
    "print(f\"loaded {len(state_dict)} parameters successfully\")\n",
    "print(\"model loading test successful\")\n",
    "\n",
    "# cleanup\n",
    "del state_dict\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T06:45:49.593301Z",
     "iopub.status.busy": "2025-10-09T06:45:49.592810Z",
     "iopub.status.idle": "2025-10-09T06:45:49.598305Z",
     "shell.execute_reply": "2025-10-09T06:45:49.597630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== phase 1 summary ===\n",
      "model: openllama-3b\n",
      "device: cuda\n",
      "pytorch save time: 6377.5 ms\n",
      "pytorch load time: 3616.3 ms\n",
      "file size: 2.73 gb\n",
      "saved to: saved_models/openllama_3b_pytorch.pth\n",
      "\n",
      "phase 1 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# phase 1 summary\n",
    "print(\"\\n=== phase 1 summary ===\")\n",
    "print(f\"model: openllama-3b\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"pytorch save time: {pytorch_save_time*1000:.1f} ms\")\n",
    "print(f\"pytorch load time: {pytorch_load_time*1000:.1f} ms\")\n",
    "print(f\"file size: {file_size:.2f} gb\")\n",
    "print(f\"saved to: {pytorch_save_path}\")\n",
    "print(\"\\nphase 1 completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
